{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import folium\n",
    "import geopandas as gpd #A more flexible package to work with geospatial data in python \n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx # networkx package \n",
    "import osmnx as ox \n",
    "import pandas as pd #Base package for data analysis and manipulation \n",
    "from pyproj import CRS #package for your projection management \n",
    "import random\n",
    "import rasterio as rio\n",
    "import rasterstats as rs\n",
    "import requests\n",
    "import shapely.geometry #python package for basic spatial operation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sections of this notebook are in order. \n",
    "The OSM settings should always be run.\n",
    "Some of the operations in this notebook require data that can be downloaded from the data folder in this repository.\n",
    "\n",
    "Downloading the AHN4 datasets can take quite some time, so make sure you have everything set up right before you do that. This also counts for downloading the networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSMnx settings\n",
    "The settings of OSMnx was changed with the settings module to extend the list of tags needed for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ox.settings.useful_tags_way.extend(['surface','footway','cycleway'])\n",
    "ox.settings.useful_tags_node.extend(['barrier'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NETWERK EXTENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_namen = ['Amsterdam', 'Den_Haag', 'Rotterdam', 'Utrecht']#set the city names\n",
    "project_gemeenten = gpd.read_file(r\"data\\boundaries\\UrbanRunner_Areas.geojson\") #download city geometries from file\n",
    "project_gemeenten[\"extent\"] = project_gemeenten.envelope.buffer(2000).envelope.to_crs(\"EPSG:4326\") #set additional geometry column of extent\n",
    "project_gemeenten.set_index('gemeentenaam', inplace=True) #set index to gemeentenaam column\n",
    "project_gemeenten.to_crs(\"EPSG:4326\", inplace=True) #project to WGS84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plot in folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_centroid = project_gemeenten.unary_union.envelope.centroid.coords[0] #point focus for folium map\n",
    "m = folium.Map(location = (map_centroid[1], map_centroid[0])) #create folium map\n",
    "#plot the extens of each network region in the folium map\n",
    "for _, r in project_gemeenten.iterrows():\n",
    "    #unbuffered convex hull\n",
    "    extent = gpd.GeoSeries(r[\"extent\"])\n",
    "    extent_j = extent.to_json()\n",
    "    extent_j = folium.GeoJson(data=extent_j, style_function=lambda x: {'fillColor': 'blue'})\n",
    "    folium.Popup(r.name).add_to(extent_j)\n",
    "    extent_j.add_to(m)\n",
    "\n",
    "    #orginal geometry\n",
    "    geom = gpd.GeoSeries(r['geometry'])\n",
    "    geom_j = geom.to_json()\n",
    "    geom_j = folium.GeoJson(data=geom_j, style_function=lambda x: {'fillColor': 'green'})\n",
    "    folium.Popup(r.name).add_to(geom_j)\n",
    "    geom_j.add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GETTING NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download network for first time\n",
    "for g in g_namen: ## beschrijven dat er twee netwerken worden gepakt: van walk en van bike, en dat deze dan worden samengevoegd met de networkx compose functie. \n",
    "    network_walk = ox.graph_from_polygon(project_gemeenten.loc[g].extent, network_type=\"walk\", retain_all=True ,simplify=False) ## we hebben gekozen voor simplify false. Beschrijven wat dit inhoudt ahv osmnx documentation. We hebben de dijkstra algoritme getest op twee netwerken, één met simplify aan en de ander met simplify uit. Er was wel een verschil, maar niet groot genoeg om simplify te gebruiken. Retain_all= true moet ook worden uitgelegd waarom. Dit is namelijk omdat we twee verschillende netwerken samenvoegen, en dat we daarna de niet verbonden edges er afgooien.\n",
    "    network_bike = ox.graph_from_polygon(project_gemeenten.loc[g].extent, network_type=\"bike\", retain_all=True, simplify=False)\n",
    "    network_both = nx.compose(network_walk, network_bike)\n",
    "    globals()[f\"{g}_network_both\"] = ox.utils_graph.get_largest_component(network_both)\n",
    "    print('finished network of', g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOWNLOADING AHN, ADDING NODE ELEVATION, ADDING EDGE GRADES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DOWNLOADING AHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahn_datavlakken = gpd.read_file(r\"data/kaartbladen_AHN4.gpkg\") #this is a spatial layers of vector tiles. Each vector tile contains download links of the AHN products for that specific tile.\n",
    "ahn_datavlakken.to_crs(\"EPSG:4326\", inplace=True)\n",
    "for g in g_namen:\n",
    "    gemeente_AHN_datavlakken = ahn_datavlakken.clip(project_gemeenten.loc[g].extent)\n",
    "    for i in range(globals()[f\"{g}_ahnvlakken\"].Name_1.count()):    \n",
    "        name = globals()[f\"{g}_ahnvlakken\"].iat[i,2]\n",
    "        response = requests.get(globals()[f\"{g}_ahnvlakken\"].iat[i,3])\n",
    "        print('done with downloading number ' + i + ' of gemeente ' + g)\n",
    "        open('data\\\\AHN4_05M_DTM\\\\'+g + '\\\\'+ name + '.zip', \"wb\").write(response.content)\n",
    "# list of paths to the different DTM tiles is made\n",
    "    globals()[f\"{g}_DTM_list\"] = []\n",
    "    for i in range(gemeente_AHN_datavlakken.Name_1.count()):    \n",
    "        name = gemeente_AHN_datavlakken.iat[i,2]\n",
    "        globals()[f\"{g}_DTM_list\"].append('data\\\\AHN4_05M_DTM\\\\'+ g + '\\\\M_'+ name +\".tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the above code results in the AHN products to be downloaded as zip. For our project, these were extracted manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen:\n",
    "    globals()[f\"{g}_network_both\"] = ox.project_graph(globals()[f\"{g}_network_both\"], to_crs=\"epsg:28992\")\n",
    "    globals()[f\"{g}_network_both\"] = ox.add_node_elevations_raster(globals()[f\"{g}_network_both\"], globals()[f\"{g}_DTM_list\"])\n",
    "    globals()[f\"{g}_nodes\"], globals()[f\"{g}_edges\"] = ox.graph_to_gdfs(globals()[f\"{g}_network_both\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDING HEAT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EnrichEdgesWithRasterInfo(edges, raster, statsprefix):\n",
    "    edges['UID'] = range(0, len(edges)) # add a temporary Unique ID column\n",
    "    e = edges.loc[:, ['UID', 'geometry']] # make subset of relevant columns\n",
    "    e_zonalstats = rs.zonal_stats(e, raster, prefix=statsprefix, geojson_out=True) # perform spatial overlay/ zonal statistics and add statistics\n",
    "    e_props = pd.DataFrame.from_dict(e_zonalstats).properties # convert to dataframe and select only the properties (results again in dictionary)\n",
    "    e_propsdf = pd.DataFrame.from_dict(list(e_props)) # convert dictionary with properties to a pandas dataframe\n",
    "    edges_updated = edges.join(other=e_propsdf.set_index('UID'), on=('UID')) # join stats to the original edges and save as updated edges\n",
    "    return edges_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess UHIi map\n",
    "rastfile = rio.open('data/RIVM_R88_20170621_gm_actueelUHI.tif')\n",
    "for g in g_namen:   \n",
    "    bboxshape = [json.loads(gpd.GeoDataFrame({\"geometry\": project_gemeenten.loc[g].extent.buffer(0.1).envelope}, \n",
    "        index=[0], crs=\"EPSG:4326\").to_crs(\"EPSG:28992\").to_json())['features'][0]['geometry']]\n",
    "    UHImap, UHImap_transform = rio.mask.mask(rastfile, shapes=bboxshape, crop=True)\n",
    "    with rio.open(f\"{g}_hittedata.tif\", 'w', driver=\"GTiff\",\n",
    "                   height=UHImap.shape[1], width=UHImap.shape[2], \n",
    "                   transform=UHImap_transform, crs=CRS.from_epsg(28992),\n",
    "                   nodata=255, dtype='float32', count= 1) as file:\n",
    "        file.write(UHImap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOTTING MAP\n",
    "UHI = rio.open('utrecht_hittedata.tif').read(1, masked=True)\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "UHIplot = ax.imshow(UHI, cmap='jet')\n",
    "ax.set_title(\"UHI map of area\", fontsize=25)\n",
    "cbar = fig.colorbar(UHIplot, fraction=0.035, pad=0.01)\n",
    "cbar.ax.get_yaxis().labelpad = 15\n",
    "cbar.ax.set_ylabel('physiologocal equivalent temperature (°C)', rotation=270)\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen:\n",
    "    globals()[f\"{g}_edges\"] = EnrichEdgesWithRasterInfo(globals()[f\"{g}_edges\"], f\"{g}_hittedata.tif\", 'UHI_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINISHING SLOPE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DOWNLOADING NETWORK AND REGRAPHING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes are downloaded to bring into QGIS. In QGIS the nodes with the missing elevation is interpolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen:\n",
    "    globals()[f\"{g}_nodes\"].to_file(f'data/graphs/retry/{g}_graph.gpkg', driver = \"GPKG\", layer= 'nodes') \n",
    "    globals()[f\"{g}_edges\"].to_file(f'data/graphs/retry/{g}_graph.gpkg', driver = \"GPKG\", layer= 'edges') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In QGIS the following steps are performed:\n",
    "1. add nodes layer;\n",
    "2. in field calculator run the expression for the added node layer:\n",
    "\n",
    "        if(elevation is Null, array_mean(overlay_nearest(layer:='nodes', filter:= elevation is not Null, expression:= elevation, limit:=5), elevation)\n",
    "3. save edits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### regraph, add edge grades, and get nodes and edges gdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen:\n",
    "    globals()[f\"{g}_nodes\"] = gpd.read_file(f'data/graphs/retry/{g}_graph.gpkg', layer= 'nodes').set_index('osmid')\n",
    "    globals()[f\"{g}_edges\"] = gpd.read_file(f'data/graphs/retry/{g}_graph.gpkg', layer= 'edges').set_index(['u','v','key'])\n",
    "    globals()[f\"{g}_network_both\"] = ox.graph_from_gdfs(globals()[f\"{g}_nodes\"],globals()[f\"{g}_edges\"])\n",
    "    globals()[f\"{g}_network_both\"] = ox.add_edge_grades(globals()[f\"{g}_network_both\"])\n",
    "    globals()[f\"{g}_nodes\"], globals()[f\"{g}_edges\"] = ox.graph_to_gdfs(globals()[f\"{g}_network_both\"], nodes=True, edges=True, node_geometry=False, fill_edge_geometry=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GREENSPACES, WATERTAPS AND OBSTACLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen:\n",
    "    globals()[f\"greenspaces_{g}\"] = ox.geometries_from_polygon(project_gemeenten.loc[g].extent, tags={'landuse':['village_green','recreation_ground','grass','forest'], 'leisure':['nature_reserve','park','garden'],'natural':['fell','heath','wood','wetland','coastline','beach']}) ## beschrijven\n",
    "    globals()[f\"greenspaces_{g}\"] = globals()[f\"greenspaces_{g}\"].loc['way'][[\"geometry\",\"natural\",\"leisure\",\"landuse\"]].to_crs(\"EPSG:28992\")\n",
    "    globals()[f\"greenspaces_{g}\"].geometry = globals()[f\"greenspaces_{g}\"].buffer(5)\n",
    "\n",
    "    globals()[f\"waterpoints_{g}\"] = ox.geometries_from_polygon(project_gemeenten.loc[g].extent, tags={'amenity':'drinking_water'})\n",
    "    globals()[f\"waterpoints_{g}\"].to_crs('EPSG:28992', inplace=True)\n",
    "    globals()[f\"waterpoints_{g}\"]['buffered'] = globals()[f\"waterpoints_{g}\"].buffer(50) ## dit moet worden onderbouwd: waarom 50 meter buffer? & in de discussie: watertappunten is lastig, want je wilt niet na 100m al langs een watertappunt lopen\n",
    "    globals()[f\"waterpoints_{g}\"].set_geometry('buffered', inplace=True, crs=\"EPSG:28992\")\n",
    "    globals()[f\"waterpoints_{g}\"] = globals()[f\"waterpoints_{g}\"][['amenity','buffered']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### adding attributes to edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen:\n",
    "    globals()[f\"{g}_edges\"] = gpd.sjoin(globals()[f\"{g}_edges\"], globals()[f\"greenspaces_{g}\"], rsuffix = \"greenspace\", how = 'left')\n",
    "    globals()[f\"{g}_edges\"] = gpd.sjoin(globals()[f\"{g}_edges\"], globals()[f\"waterpoints_{g}\"], rsuffix = \"waterpoints\", how = 'left')\n",
    "    nodes_disruptions = globals()[f\"{g}_nodes\"][['highway', 'barrier']]\n",
    "    globals()[f\"{g}_edges\"] = globals()[f\"{g}_edges\"].join(nodes_disruptions.rename_axis('u'), how = 'left', rsuffix = \"_obstnodes_u\")\n",
    "    globals()[f\"{g}_edges\"] = globals()[f\"{g}_edges\"].join(nodes_disruptions.rename_axis('v'), how = 'left', rsuffix = \"_obstnodes_v\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECKING THE RESULTING NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CHECKING UNIQUE VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['UHI_mean', 'barrier', 'highway', 'cycleway', 'footway', 'surface', 'highway_obstnodes_u','barrier']\n",
    "for v in attributes:\n",
    "    globals()[f\"{v}_uniques\"] = []\n",
    "    for g in g_namen:\n",
    "        globals()[f\"{v}_uniques\"].extend(globals()[f\"{g}_edges\"][v].unique())\n",
    "    globals()[f\"{v}_uniques\"] = list(set(globals()[f\"{v}_uniques\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### checking occurences of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occurence_value(value, variable):\n",
    "    for g in g_namen:\n",
    "        edges = globals()[f\"{g}_edges\"]\n",
    "        print(value, 'occurs', len(edges[edges[variable] == value]), 'times for the variable', variable, 'in the area of', g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurence_value('asphalt', 'surface')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COST FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen:\n",
    "    edges = globals()[f\"{g}_edges\"] \n",
    "    #waterpoints\n",
    "    edges['cost_waterpoints'] = 1\n",
    "    edges['cost_waterpoints'][edges.amenity == 'drinking_water'] = 0\n",
    "    #UHI\n",
    "    edges['cost_UHI'][edges['UHI_mean'] < 0] == 0 \n",
    "    edges['cost_UHI'] = edges['UHI_mean'] / 3 #3 comes from the highest UHI_mean value of all the areas\n",
    "    #Greenspace\n",
    "    edges['cost_greenspace'] = 0\n",
    "    edges['cost_greenspace'][(edges['landuse'].isnull()) & (edges['leisure'].isnull()) & (edges['natural'].isnull())] = 1\n",
    "    #slope\n",
    "    edges['cost_grade'] = 0\n",
    "    edges['cost_grade'][(edges['grade'] > 0)] = edges['grade'] / 5.982 #5.982 is the highest grade value of all the areas\n",
    "    #disruptions\n",
    "    edges['cost_disruptions'] = 0\n",
    "    edges['cost_disruptions'][(edges['footway'] == 'crossing') |(edges['cycleway'] == 'crossing') |(edges['barrier_obstnodes_u'] == 'gate')|(edges['barrier_obstnodes_u'] == 'wicket_gate')|(edges['barrier_obstnodes_u'] == 'stile')|(edges['barrier_obstnodes_u'] == 'turnstile')|(edges['barrier_obstnodes_u'] == 'full-height_turnstile')] = 0.5\n",
    "    edges['cost_disruptions'][(edges['highway_obstnodes_u'] == 'traffic_signals')|(edges['highway_obstnodes_v'] == 'traffic_signals')] = 1\n",
    "    #surface_unpaved\n",
    "    edges['cost_surface_prefunpaved'] = 0.5\n",
    "    edges['cost_surface_prefunpaved'][edges['surface'].isin(['dirt;grass','grass_paver','mud','dirt','unpaved','wetland','clay','grass','sand','ground','earth','woodchips','reed_bed','soil'])] = 0\n",
    "    edges['cost_surface_prefunpaved'][edges['surface'].isin(['dirt;gravel','crushed_shells','gravel','fine_gravel','shells','schelpen','rubber'])] = 0.25\n",
    "    edges['cost_surface_prefunpaved'][edges['surface'].isin(['compacted','wood','resin','rasin'])] = 0.75\n",
    "    edges['cost_surface_prefunpaved'][edges['surface'].isin(['concrete-slabs','steel','paving_stones;asphalt','concrete:lanes','concrete_slab','concrete_slabs','concrete:deconstructed_czech_hedgehogs','paved','asphalt','metal','marble','concrete:plates','concrete:tiles','concrete','zinc','bricks','brick','tiles','concrete_tiles:30','paving_stones:20','paving_stones:30','metal_grid','asphalt;paving_stones:30x30','stepping_stones','paving_stones','stone','pebblestone','cobblestone','unhewn_cobblestone'])] = 1\n",
    "    #surface_paved\n",
    "    edges['cost_surface_prefpaved'] = 0.5\n",
    "    edges['cost_surface_prefpaved'][edges['surface'].isin(['dirt;grass','grass_paver','mud','dirt','unpaved','wetland','clay','grass','sand','ground','earth','woodchips','reed_bed','soil'])] = 1\n",
    "    edges['cost_surface_prefpaved'][edges['surface'].isin(['dirt;gravel','crushed_shells','gravel','fine_gravel','shells','schelpen','rubber'])] = 0.75\n",
    "    edges['cost_surface_prefpaved'][edges['surface'].isin(['compacted','wood','resin','rasin'])] = 0.25\n",
    "    edges['cost_surface_prefpaved'][edges['surface'].isin(['concrete-slabs','steel','paving_stones;asphalt','concrete:lanes','concrete_slab','concrete_slabs','concrete:deconstructed_czech_hedgehogs','paved','asphalt','metal','marble','concrete:plates','concrete:tiles','concrete','zinc','bricks','brick','tiles','concrete_tiles:30','paving_stones:20','paving_stones:30','metal_grid','asphalt;paving_stones:30x30','stepping_stones','paving_stones','stone','pebblestone','cobblestone','unhewn_cobblestone'])] = 0\n",
    "        \n",
    "    globals()[f\"{g}_edges\"] = edges\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CALCULATING FINAL COST COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen:\n",
    "    edges = globals()[f\"{g}_edges\"]\n",
    "    edges['final_cost_willemijn'] = ((0.7*edges['cost_UHI']) + (0.9*edges['cost_greenspace']) + (0.5*edges['cost_grade']) + (0.1*edges['cost_disruptions']) + (0.3*edges['cost_surface_prefpaved'])) * edges['length']\n",
    "    edges['final_cost_rachid'] = ((0.5*edges['cost_waterpoints']) + (0.7*edges['cost_UHI']) + (0.9*edges['cost_greenspace']) + (0.3*edges['cost_disruptions']) + (0.1*edges['cost_surface_prefunpaved'])) * edges['length']\n",
    "    globals()[f\"{g}_edges\"] = edges "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SAVE GRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen:\n",
    "    globals()[f\"{g}_edges\"].to_file(f'data/graphs/retry/with_costs/{g}_edges_with_costs.gpkg', driver = \"GPKG\", layer= 'edges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utrecht_centrum_extent = gpd.GeoSeries(\n",
    "    shapely.geometry.Polygon([(135874.0,456545.4), (137381.2 ,456742.0), (137459.9 ,454749.8), (136018.1, 454703.9)]).envelope, \n",
    "    crs= \"EPSG:28992\").to_crs(\"epsg:4326\") #small part of Utrecht is chosen as\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify True\n",
    "Utrecht_network_walk = ox.graph_from_polygon(Utrecht_centrum_extent[0], network_type=\"walk\")\n",
    "Utrecht_network_bike = ox.graph_from_polygon(Utrecht_centrum_extent[0], network_type=\"bike\")\n",
    "Utrecht_network_both_test_simplified = nx.compose(Utrecht_network_walk, Utrecht_network_bike)\n",
    "\n",
    "#simply false\n",
    "Utrecht_network_walk = ox.graph_from_polygon(Utrecht_centrum_extent[0], network_type=\"walk\", simplify=False)\n",
    "Utrecht_network_bike = ox.graph_from_polygon(Utrecht_centrum_extent[0], network_type=\"bike\", simplify=False)\n",
    "Utrecht_network_both_test_unsimplified = nx.compose(Utrecht_network_walk, Utrecht_network_bike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utrecht_centrum_extent[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_XY_in_polygon(poly): ## manier om XY coordinaten te pakken binnen een polygon\n",
    "    minx, miny, maxx, maxy = poly.bounds\n",
    "    while True:\n",
    "        p = shapely.geometry.Point(random.uniform(minx, maxx), random.uniform(miny, maxy))\n",
    "        if poly.contains(p):\n",
    "            return p.coords[0][0], p.coords[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list = []\n",
    "Y_list = []    \n",
    "for i in range(100):\n",
    "    x, y = get_random_XY_in_polygon(Utrecht_centrum_extent[0])\n",
    "    X_list.append(x)\n",
    "    Y_list.append(y)\n",
    "\n",
    "#getting nodes from the random XY sets\n",
    "#UNSIMPLIFIED\n",
    "Utrecht_random_nodes_unsimplified = ox.nearest_nodes(Utrecht_network_both_test_unsimplified, X_list, Y_list, return_dist=True)\n",
    "Utrecht_source_nodes_unsimplified = Utrecht_random_nodes_unsimplified[0][:50]\n",
    "Utrecht_goal_nodes_unsimplified   = Utrecht_random_nodes_unsimplified[0][50:100]\n",
    "#SIMPLIFIED\n",
    "Utrecht_random_nodes_simplified = ox.nearest_nodes(Utrecht_network_both_test_simplified, X_list, Y_list, return_dist=True)\n",
    "Utrecht_source_nodes_simplified = Utrecht_random_nodes_simplified[0][:50]\n",
    "Utrecht_goal_nodes_simplified   =   Utrecht_random_nodes_simplified[0][50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating shortest route of the source and goal nodes for both simplified and unsimplified network, then comparing computing time\n",
    "#UNSIMPLIFIED\n",
    "start_time = time.time()\n",
    "random_shortest_routes = ox.distance.shortest_path(\n",
    "    Utrecht_network_both_test_unsimplified, Utrecht_source_nodes_unsimplified, Utrecht_goal_nodes_unsimplified, weight='length', cpus=1)\n",
    "\n",
    "print(\"gathering random routes for unsimplified network takes %s seconds\" % (time.time() - start_time))\n",
    "randomroute_time_unsimplified = (time.time() - start_time)\n",
    "\n",
    "#SIMPLIFIED\n",
    "start_time = time.time()\n",
    "random_shortest_routes_simplified = ox.distance.shortest_path(\n",
    "    Utrecht_network_both_test_simplified, Utrecht_source_nodes_simplified, Utrecht_goal_nodes_simplified, weight='length', cpus=1)\n",
    "    \n",
    "print(\"gathering random routes for simplified network takes %s seconds\" % (time.time() - start_time))\n",
    "randomroute_time_simplified = (time.time() - start_time)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "74868beb4ec27fd26d4cf7f91852fff1668a283e4d2efd66b1e888f5eef2fc2a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 ('ox')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
