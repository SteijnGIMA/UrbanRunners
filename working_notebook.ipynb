{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import folium\n",
    "import geopandas as gpd #A more flexible package to work with geospatial data in python \n",
    "from itertools import product\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx # networkx package \n",
    "import numpy as np\n",
    "from osgeo import ogr #GDAL package \n",
    "import osmnx as ox \n",
    "import pandas as pd #Base package for data analysis and manipulation \n",
    "from pyproj import CRS #package for your projection management \n",
    "import random\n",
    "import rasterio as rio\n",
    "from rasterio import mask\n",
    "import rasterstats as rs\n",
    "import requests\n",
    "import shapely.geometry #python package for basic spatial operation \n",
    "import xml.dom.minidom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sections of this notebook are in order. \n",
    "The OSM settings should always be run.\n",
    "Some of the operations in this notebook require data that can be downloaded from the data folder in this repository.\n",
    "\n",
    "Downloading the AHN4 datasets can take quite some time, so make sure you have everything set up right before you do that. This also counts for downloading the networks.\n",
    "\n",
    "The section with OSMOSIS can be ignored for now, but might be necessary for later if the downloading of network via OSMnx fails for some reason.\n",
    "\n",
    "Adding grid value section is a work in progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSM settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adds surface as extra tag for the edges (THIS IS A REQUIRED STEP BEFORE YOU DOWNLOAD THE NETWORK)\n",
    "ox.settings.useful_tags_way.extend(['surface','footway','cycleway', 'crossing', 'barrier'])\n",
    "ox.settings.useful_tags_node.extend(['barrier'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NETWERK EXTENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the city names\n",
    "g_namen = ['Amsterdam', 'Rotterdam', \"Utrecht\", \"Den_Haag\"]\n",
    "#g_namen = [\"Den_Haag\"]\n",
    "\n",
    "#download city geometries from file\n",
    "project_gemeenten = gpd.read_file(r\"data\\boundaries\\UrbanRunner_Areas.geojson\") ## dit komt van de cbs gemeentegrenzen. Beschrijven bij data\n",
    "\n",
    "#set additional geometry column of extent\n",
    "project_gemeenten[\"extent\"] = project_gemeenten.envelope.buffer(2000).envelope.to_crs(\"EPSG:4326\")\n",
    "\n",
    "#set index to gemeentenaam column\n",
    "project_gemeenten.set_index('gemeentenaam', inplace=True)\n",
    "\n",
    "#project to WGS84\n",
    "project_gemeenten.to_crs(\"EPSG:4326\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### network extent for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utrecht_centrum_extent = gpd.GeoSeries(shapely.geometry.Polygon([(135874.0,456545.4), (137381.2 ,456742.0), (137459.9 ,454749.8), (136018.1, 454703.9)]).envelope, crs= \"EPSG:28992\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plot in folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_centroid = project_gemeenten.unary_union.envelope.centroid.coords[0]\n",
    "\n",
    "#create folium map\n",
    "m = folium.Map(location = (map_centroid[1], map_centroid[0]))\n",
    "#plot the convexhulls of each network region in the folium map\n",
    "for _, r in project_gemeenten.iterrows():\n",
    "    #unbuffered convex hull\n",
    "    extent = gpd.GeoSeries(r[\"extent\"])\n",
    "    extent_j = extent.to_json()\n",
    "    extent_j = folium.GeoJson(data=extent_j,\n",
    "                           style_function=lambda x: {'fillColor': 'blue'})\n",
    "    folium.Popup(r.name).add_to(extent_j)\n",
    "    extent_j.add_to(m)\n",
    "\n",
    "    #orginal geometry\n",
    "    geom = gpd.GeoSeries(r['geometry'])\n",
    "    geom_j = geom.to_json()\n",
    "    geom_j = folium.GeoJson(data=geom_j,\n",
    "                           style_function=lambda x: {'fillColor': 'green'})\n",
    "    folium.Popup(r.name).add_to(geom_j)\n",
    "    geom_j.add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GETTING NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download network for first time\n",
    "for g in g_namen: ## beschrijven dat er twee netwerken worden gepakt: van walk en van bike, en dat deze dan worden samengevoegd met de networkx compose functie. \n",
    "    print('starting with', g)\n",
    "    globals()[f\"{g}_network_walk\"] = ox.graph_from_polygon(project_gemeenten.loc[g].extent, network_type=\"walk\", retain_all=True ,simplify=False) ## we hebben gekozen voor simplify false. Beschrijven wat dit inhoudt ahv osmnx documentation. We hebben de dijkstra algoritme getest op twee netwerken, één met simplify aan en de ander met simplify uit. Er was wel een verschil, maar niet groot genoeg om simplify te gebruiken. Retain_all= true moet ook worden uitgelegd waarom. Dit is namelijk omdat we twee verschillende netwerken samenvoegen, en dat we daarna de niet verbonden edges er afgooien.\n",
    "    print('done with walking graph of', g)\n",
    "    globals()[f\"{g}_network_bike\"] = ox.graph_from_polygon(project_gemeenten.loc[g].extent, network_type=\"bike\", retain_all=True, simplify=False)\n",
    "    print('done with biking graph of', g)\n",
    "    globals()[f\"{g}_network_both\"] = nx.compose(globals()[f\"{g}_network_walk\"], globals()[f\"{g}_network_bike\"])\n",
    "    globals()[f\"{g}_network_both\"] = ox.utils_graph.get_largest_component(globals()[f\"{g}_network_both\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NETWORK FROM GEOPACKAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphs_from_geopackage(gemeente, type):\n",
    "    \"\"\"\n",
    "    gemeente (str): name of gemeente\n",
    "    type (str): 'gdf'  \n",
    "                'graph'\n",
    "                'both' \n",
    "\n",
    "    return: MultiDiGraph AND/OR nodes and edges (as gdf)\n",
    "    \"\"\"  \n",
    "    fp = 'data/graphs/reprojected/'+gemeente+'_Graph.gpkg'\n",
    "    gdf_nodes = gpd.read_file(fp, layer='nodes').set_index('osmid')\n",
    "    gdf_edges = gpd.read_file(fp, layer='edges').set_index(['u', 'v', 'key'])\n",
    "    assert gdf_nodes.index.is_unique and gdf_edges.index.is_unique\n",
    "    if type == 'gdf':\n",
    "        return gdf_nodes, gdf_edges\n",
    "    elif type == 'graph':\n",
    "        # convert the node/edge GeoDataFrames to a MultiDiGraph\n",
    "        graph_attrs = {'crs': 'epsg:4326', 'simplified': False}\n",
    "        G = ox.graph_from_gdfs(gdf_nodes, gdf_edges, graph_attrs)\n",
    "        return G\n",
    "    elif type == 'both':\n",
    "        # convert the node/edge GeoDataFrames to a MultiDiGraph\n",
    "        graph_attrs = {'crs': 'epsg:4326', 'simplified': False}\n",
    "        G = ox.graph_from_gdfs(gdf_nodes, gdf_edges, graph_attrs)\n",
    "        return G, gdf_nodes, gdf_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### for testing (small network extent within Utrecht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small network for testing\n",
    "# simplify True\n",
    "for g in [\"Utrecht\"]:\n",
    "    globals()[f\"{g}_network_walk_test\"] = ox.graph_from_polygon(project_gemeenten.loc[g].extent, network_type=\"walk\")\n",
    "    print('done with walking network')\n",
    "    globals()[f\"{g}_network_bike_test\"] = ox.graph_from_polygon(project_gemeenten.loc[g].extent, network_type=\"bike\")\n",
    "    print('done with biking network')\n",
    "    globals()[f\"{g}_network_both_test_simplified\"] = nx.compose(globals()[f\"{g}_network_walk_test\"], globals()[f\"{g}_network_bike_test\"])\n",
    "\n",
    "#simplify false\n",
    "for g in [\"Utrecht\"]:\n",
    "    globals()[f\"{g}_network_walk_test\"] = ox.graph_from_polygon(project_gemeenten.loc[g].extent, network_type=\"walk\", simplify=False)\n",
    "    print('done with walking network')\n",
    "    globals()[f\"{g}_network_bike_test\"] = ox.graph_from_polygon(project_gemeenten.loc[g].extent, network_type=\"bike\", simplify=False)\n",
    "    print('done with biking network')\n",
    "    globals()[f\"{g}_network_both_test\"] = nx.compose(globals()[f\"{g}_network_walk_test\"], globals()[f\"{g}_network_bike_test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### testing with small extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallextent_walk = ox.graph_from_polygon(Utrecht_centrum_extent.to_crs(\"EPSG:4326\")[0], network_type=\"walk\")\n",
    "print('done with walking network')\n",
    "smallextent_bike = ox.graph_from_polygon(Utrecht_centrum_extent.to_crs(\"EPSG:4326\")[0], network_type=\"bike\")\n",
    "print('done with biking network')\n",
    "smallextent_both = nx.compose(smallextent_walk, smallextent_bike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallextent_edges = ox.graph_to_gdfs(smallextent_both, nodes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOWNLOADING AHN, ADDING NODE ELEVATION, ADDING EDGE GRADES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DOWNLOADING AHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahn_datavlakken = gpd.read_file(r\"data/kaartbladen_AHN4.gpkg\") ## AHN4 data beschrijven in methode. Kaartbladen_AHN4 bestaan uit tiles met download links naar verschillende varianten van AHN4. Wij gebruiken AHN4_0.5M_DTM. Beschrijven waarom deze wordt gepakt.\n",
    "ahn_datavlakken.to_crs(\"EPSG:4326\", inplace=True)\n",
    "for g in g_namen:\n",
    "    globals()[f\"{g}_ahnvlakken\"]=ahn_datavlakken.clip(project_gemeenten.loc[g].extent) ## Beschrijven in de methode hoe de juiste tiles wordt gekozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gemeente in g_namen:\n",
    "    globals()[f\"{gemeente}_ahnvlakken\"]=ahn_datavlakken.clip(project_gemeenten.loc[gemeente].extent)\n",
    "    for i in range(globals()[f\"{gemeente}_ahnvlakken\"].Name_1.count()):    \n",
    "        name = globals()[f\"{gemeente}_ahnvlakken\"].iat[i,2]\n",
    "\n",
    "        response = requests.get(globals()[f\"{gemeente}_ahnvlakken\"].iat[i,3])\n",
    "        print('done with downloading number ' + i + ' of gemeente ' + gemeente)\n",
    "        open('C:\\\\Users\\\\danny\\\\Documents\\\\persoonlijk\\\\GIMA\\\\modules\\\\module 6\\\\data\\\\AHN4_05M_DTM\\\\'+gemeente + '\\\\'+ name + '.zip', \"wb\").write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADDING NODE ELEVATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen:\n",
    "    globals()[f\"{g}_DTM_list\"] = []\n",
    "    for i in range(globals()[f\"{g}_ahnvlakken\"].Name_1.count()):    \n",
    "        name = globals()[f\"{g}_ahnvlakken\"].iat[i,2]\n",
    "        globals()[f\"{g}_DTM_list\"].append('C:\\\\Users\\\\danny\\\\Documents\\\\persoonlijk\\\\GIMA\\\\modules\\\\module 6\\\\data\\\\AHN4_05M_DTM\\\\'+ g + '\\\\M_'+ name +\".tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen:\n",
    "    globals()[f\"{g}_network_both\"] = ox.project_graph(globals()[f\"{g}_network_both\"], to_crs=\"epsg:28992\")\n",
    "    globals()[f\"{g}_network_both\"] = ox.add_node_elevations_raster(globals()[f\"{g}_network_both\"], globals()[f\"{g}_DTM_list\"])\n",
    "    globals()[f\"{g}_nodes\"], globals()[f\"{g}_edges\"] = ox.graph_to_gdfs(globals()[f\"{g}_network_both\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDING HEAT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EnrichEdgesWithRasterInfo(edges, raster, statsprefix):\n",
    "    edges['UID'] = range(0, len(edges)) # add a temporary Unique ID column\n",
    "    e = edges.loc[:, ['UID', 'geometry']] # make subset of relevant columns\n",
    "    e_zonalstats = rs.zonal_stats(e, raster, prefix=statsprefix, geojson_out=True) # perform spatial overlay/ zonal statistics and add statistics\n",
    "    e_props = pd.DataFrame.from_dict(e_zonalstats).properties # convert to dataframe and select only the properties (results again in dictionary)\n",
    "    e_propsdf = pd.DataFrame.from_dict(list(e_props)) # convert dictionary with properties to a pandas dataframe\n",
    "    edges_updated = edges.join(other=e_propsdf.set_index('UID'), on=('UID')) # join stats to the original edges and save as updated edges\n",
    "    return edges_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess UHIi map\n",
    "rastfile = rio.open('data/RIVM_R88_20170621_gm_actueelUHI.tif')\n",
    "for g in g_namen:   \n",
    "    bboxshape = [json.loads(gpd.GeoDataFrame({\"geometry\": project_gemeenten.loc[g].extent.buffer(0.1).envelope}, \n",
    "        index=[0], crs=\"EPSG:4326\").to_crs(\"EPSG:28992\").to_json())['features'][0]['geometry']]\n",
    "    UHImap, UHImap_transform = rio.mask.mask(rastfile, shapes=bboxshape, crop=True)\n",
    "    with rio.open(f\"{g}_hittedata.tif\", 'w', driver=\"GTiff\",\n",
    "                   height=UHImap.shape[1], width=UHImap.shape[2], \n",
    "                   transform=UHImap_transform, crs=CRS.from_epsg(28992),\n",
    "                   nodata=255, dtype='float32', count= 1) as file:\n",
    "        file.write(UHImap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOTTING MAP\n",
    "UHI = rio.open('utrecht_hittedata.tif').read(1, masked=True)\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "UHIplot = ax.imshow(UHI, cmap='jet')\n",
    "ax.set_title(\"UHI map of area\", fontsize=25)\n",
    "cbar = fig.colorbar(UHIplot, fraction=0.035, pad=0.01)\n",
    "cbar.ax.get_yaxis().labelpad = 15\n",
    "cbar.ax.set_ylabel('physiologocal equivalent temperature (°C)', rotation=270)\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen:\n",
    "    globals()[f\"{g}_edges\"] = EnrichEdgesWithRasterInfo(globals()[f\"{g}_edges\"], f\"{g}_hittedata.tif\", 'UHI_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINISHING SLOPE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DOWNLOADING NETWORK AND REGRAPHING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes are downloaded to bring into QGIS. In QGIS the nodes with the missing elevation is interpolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen:\n",
    "    globals()[f\"{g}_nodes\"].to_file(f'data/graphs/retry/{g}_graph.gpkg', driver = \"GPKG\", layer= 'nodes') \n",
    "    globals()[f\"{g}_edges\"].to_file(f'data/graphs/retry/{g}_graph.gpkg', driver = \"GPKG\", layer= 'edges') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In QGIS the following steps are performed:\n",
    "1. add nodes layer;\n",
    "2. in field calculator run the expression for the added node layer:\n",
    "\n",
    "        if(elevation is Null, array_mean(overlay_nearest(layer:='nodes', filter:= elevation is not Null, expression:= elevation, limit:=5), elevation)\n",
    "3. save edits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### regraph, add edge grades, and get nodes and edges gdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen:\n",
    "    globals()[f\"{g}_nodes\"] = gpd.read_file(f'data/graphs/retry/{g}_graph.gpkg', layer= 'nodes').set_index('osmid')\n",
    "    globals()[f\"{g}_edges\"] = gpd.read_file(f'data/graphs/retry/{g}_graph.gpkg', layer= 'edges').set_index(['u','v','key'])\n",
    "    globals()[f\"{g}_network_both\"] = ox.graph_from_gdfs(globals()[f\"{g}_nodes\"],globals()[f\"{g}_edges\"])\n",
    "    globals()[f\"{g}_network_both\"] = ox.add_edge_grades(globals()[f\"{g}_network_both\"])\n",
    "    globals()[f\"{g}_nodes\"], globals()[f\"{g}_edges\"] = ox.graph_to_gdfs(globals()[f\"{g}_network_both\"], nodes=True, edges=True, node_geometry=False, fill_edge_geometry=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GREENSPACES, WATERTAPS AND OBSTACLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen:\n",
    "    globals()[f\"greenspaces_{g}\"] = ox.geometries_from_polygon(project_gemeenten.loc[g].extent, tags={'landuse':['village_green','recreation_ground','grass','forest'], 'leisure':['nature_reserve','park','garden'],'natural':['fell','heath','wood','wetland','coastline','beach']}) ## beschrijven\n",
    "    globals()[f\"greenspaces_{g}\"] = globals()[f\"greenspaces_{g}\"].loc['way'][[\"geometry\",\"natural\",\"leisure\",\"landuse\"]].to_crs(\"EPSG:28992\")\n",
    "    globals()[f\"greenspaces_{g}\"].geometry = globals()[f\"greenspaces_{g}\"].buffer(5)\n",
    "\n",
    "    globals()[f\"waterpoints_{g}\"] = ox.geometries_from_polygon(project_gemeenten.loc[g].extent, tags={'amenity':'drinking_water'})\n",
    "    globals()[f\"waterpoints_{g}\"].to_crs('EPSG:28992', inplace=True)\n",
    "    globals()[f\"waterpoints_{g}\"]['buffered'] = globals()[f\"waterpoints_{g}\"].buffer(50) ## dit moet worden onderbouwd: waarom 50 meter buffer? & in de discussie: watertappunten is lastig, want je wilt niet na 100m al langs een watertappunt lopen\n",
    "    globals()[f\"waterpoints_{g}\"].set_geometry('buffered', inplace=True, crs=\"EPSG:28992\")\n",
    "    globals()[f\"waterpoints_{g}\"] = globals()[f\"waterpoints_{g}\"][['amenity','buffered']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### adding to node and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen:\n",
    "    globals()[f\"{g}_edges\"] = gpd.sjoin(globals()[f\"{g}_edges\"], globals()[f\"greenspaces_{g}\"], rsuffix = \"greenspace\", how = 'left')\n",
    "    globals()[f\"{g}_edges\"] = gpd.sjoin(globals()[f\"{g}_edges\"], globals()[f\"waterpoints_{g}\"], rsuffix = \"waterpoints\", how = 'left')\n",
    "    nodes_disruptions = globals()[f\"{g}_nodes\"][['highway', 'barrier']]\n",
    "    globals()[f\"{g}_edges\"] = globals()[f\"{g}_edges\"].join(nodes_disruptions.rename_axis('u'), how = 'left', rsuffix = \"_obstnodes_u\")\n",
    "    globals()[f\"{g}_edges\"] = globals()[f\"{g}_edges\"].join(nodes_disruptions.rename_axis('v'), how = 'left', rsuffix = \"_obstnodes_v\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVING NETWORKS TO GEOPACKAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving networks\n",
    "def saving_networks(network,g):\n",
    "    ox.save_graph_geopackage(network, filepath='data\\\\graphs\\\\' + g + '_Graph_complete2.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen:\n",
    "    saving_networks(test_Utrecht_graph, g = g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECKING SOME OF THE VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CHECKING UNIQUE VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utrecht_edges.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['UHI_mean', 'barrier', 'highway', 'cycleway', 'footway', 'surface', 'highway_obstnodes_u','barrier']\n",
    "for v in attributes:\n",
    "    globals()[f\"{v}_uniques\"] = []\n",
    "    for g in g_namen:\n",
    "        globals()[f\"{v}_uniques\"].extend(globals()[f\"{g}_edges\"][v].unique())\n",
    "    globals()[f\"{v}_uniques\"] = list(set(globals()[f\"{v}_uniques\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(highway_obstnodes_u_uniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### checking occurences of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occurence_value(value, variable):\n",
    "    for g in g_namen:\n",
    "        edges = globals()[f\"{g}_edges\"]\n",
    "        print(value, 'occurs', len(edges[edges[variable] == value]), 'times for the variable', variable, 'in the area of', g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurence_value('rasin', 'surface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utrecht_edges[Utrecht_edges['surface'] == 'rasin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_XY_in_polygon(poly): ## manier om XY coordinaten te pakken binnen een polygon\n",
    "    minx, miny, maxx, maxy = poly.bounds\n",
    "    while True:\n",
    "        p = shapely.geometry.Point(random.uniform(minx, maxx), random.uniform(miny, maxy))\n",
    "        if poly.contains(p):\n",
    "            return p.coords[0][0], p.coords[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of nodes\n",
    "n_nodes = 100\n",
    "\n",
    "#getting random nodes\n",
    "for g in g_namen:\n",
    "    X_list = []\n",
    "    Y_list = []    \n",
    "    for i in range(n_nodes):\n",
    "        x, y = get_random_XY_in_polygon(project_gemeenten.to_crs('epsg:28992').loc[g].geometry)\n",
    "        X_list.append(x)\n",
    "        Y_list.append(y)\n",
    "    globals()[f\"{g}_random_nodes\"] = ox.nearest_nodes(Utrecht_network_both, X_list, Y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.dijkstra_path(G=Utrecht_network_both, source=Utrecht_random_nodes[3], target=Utrecht_random_nodes[1], weight='final_cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting nodes from the random XY sets\n",
    "#UNSIMPLIFIED\n",
    "\n",
    "source_nodes = globals()[f\"{g}_random_nodes\"][0][:50]\n",
    "goal_nodes = globals()[f\"{g}_random_nodes\"][0][50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_shortest_routes = ox.distance.shortest_path(\n",
    "    test_Utrecht_graph, source_nodes, goal_nodes, weight='final_cost', cpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_shortest_routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test: comparing simplified with nonsimplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in [\"Utrecht\"]:\n",
    "    X_list = []\n",
    "    Y_list = []    \n",
    "    for i in range(100):\n",
    "        x, y = get_random_XY_in_polygon(project_gemeenten.loc[g].geometry)\n",
    "        X_list.append(x)\n",
    "        Y_list.append(y)\n",
    "    #random_nodes = ox.nearest_nodes(, X_list, Y_list, return_dist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting nodes from the random XY sets\n",
    "#UNSIMPLIFIED\n",
    "random_nodes = ox.nearest_nodes(u_network_both, X_list, Y_list, return_dist=True)\n",
    "source_nodes = random_nodes[0][:50]\n",
    "goal_nodes = random_nodes[0][50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting nodes from the random XY sets\n",
    "#UNSIMPLIFIED\n",
    "Utrecht_random_nodes = ox.nearest_nodes(Utrecht_network_both_test, X_list, Y_list, return_dist=True)\n",
    "Utrecht_source_nodes = Utrecht_random_nodes[0][:50]\n",
    "Utrecht_goal_nodes   =    Utrecht_random_nodes[0][50:100]\n",
    "#SIMPLIFIED\n",
    "Utrecht_random_nodes_simplified = ox.nearest_nodes(Utrecht_network_both_test_simplified, X_list, Y_list, return_dist=True)\n",
    "Utrecht_source_nodes_simplified = Utrecht_random_nodes_simplified[0][:50]\n",
    "Utrecht_goal_nodes_simplified   =   Utrecht_random_nodes_simplified[0][50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating shortest route of the source and goal nodes\n",
    "#UNSIMPLIFIED\n",
    "start_time = time.time()\n",
    "random_shortest_routes = ox.distance.shortest_path(\n",
    "    Utrecht_network_both_test, Utrecht_source_nodes, Utrecht_goal_nodes, weight='length', cpus=1)\n",
    "\n",
    "print(\"gathering random routes for unsimplified network takes %s seconds\" % (time.time() - start_time))\n",
    "randomroute_time = (time.time() - start_time)\n",
    "\n",
    "#SIMPLIFIED\n",
    "start_time = time.time()\n",
    "random_shortest_routes_simplified = ox.distance.shortest_path(\n",
    "    Utrecht_network_both_test_simplified, Utrecht_source_nodes_simplified, Utrecht_goal_nodes_simplified, weight='length', cpus=1)\n",
    "    \n",
    "print(\"gathering random routes for simplified network takes %s seconds\" % (time.time() - start_time))\n",
    "randomroute_time_simplified = (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ox.folium.plot_route_folium(u_network_both, random_shortest_routes[2], route_map=None, popup_attribute=None, tiles='cartodbpositron', zoom=1, fit_bounds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ox.folium.plot_route_folium(u_network_both_simplified, random_shortest_routes_simplified[2], route_map=None, popup_attribute=None, tiles='cartodbpositron', zoom=1, fit_bounds=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for ways"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "osmosis --read-xml utrecht-latest.osm --tf accept-ways highway=* --tf reject-ways highway=abandoned,bus_guideway,construction,motor,proposed,planned,raceway,cycleway service=private foot=no access=private --tf reject-relations --used-node --write-xml utrecht_walk.osm\n",
    "osmosis --read-xml utrecht-latest.osm --tf accept-ways highway=* --tf reject-ways highway=abandoned,bus_guideway,construction,motor,proposed,planned,raceway,corridor,elevator,escalator,footway,steps service=private bicycle=no access=private --tf reject-relations --used-node --write-xml utrecht_bike.osm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "osmosis --read-pbf utrecht-latest.osm.pbf --tf accept-ways highway=* --used-node --write-xml highways.osm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "osmosis --read-xml utrecht_combination.osm --bounding-box left=4.940698126246746 bottom=52.00802236935806 right=5.224513606042486 top=52.16049477581679 --write-xml utrecht_combination_extent.osm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for greenspaces\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "osmosis --read-xml utrecht-latest.osm --bounding-box left=4.940698126246746 bottom=52.00802236935806 right=5.224513606042486 top=52.16049477581679 --tf accept-ways landuse=village_green,recreation_ground,grass,forest leisure=park,garden,nature_reserve natural=fell,heath,wood --tf accept-nodes landuse=village_green,recreation_ground,grass,forest leisure=park,garden,nature_reserve natural=fell,heath,wood --tf reject-relations --write-xml utrecht_greenspaces.osm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COUNTING greenspace per type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = xml.dom.minidom.parse(r\"data/OSM/utrecht_greenspaces.osm\")\n",
    "\n",
    "ways = doc.getElementsByTagName('way')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wayWithLanduse = []\n",
    "wayWithLeisure = []\n",
    "wayWithNatural = []\n",
    "for way in ways:\n",
    "    for way_child in way.childNodes:\n",
    "        if type(way_child) == xml.dom.minidom.Text:\n",
    "            continue\n",
    "        elif way_child.hasAttribute('k'):\n",
    "            if way_child.getAttribute('k') == 'landuse': \n",
    "                wayWithLanduse.append(way)\n",
    "            elif way_child.getAttribute('k') == 'leisure':\n",
    "                wayWithLeisure.append(way)\n",
    "            elif way_child.getAttribute('k') == 'natural':\n",
    "                wayWithNatural.append(way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(wayWithLeisure), len(wayWithLanduse), len(wayWithNatural))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COST FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utrecht_nodes, Utrecht_edges = graphs_from_geopackage('Utrecht', 'gdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utrecht_edges_dropped = Utrecht_edges.drop(columns=[\"lanes\",\"width\",\"est_width\", \"ref\",'index_waterpoints0', 'index_waterpoints1','UID','maxspeed', 'bridge', 'access'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SAVE GDFS WITHOUT COST COLUMNS TO GEOPACKAGE\n",
    "These will be baseline edge and nodes that will be reused for different personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen: \n",
    "    globals()[f\"{g}_edges\"].to_file(f'data/graphs/retry/baseline_edges/{g}_edges_baseline.gpkg', driver = \"GPKG\", layer= 'edges') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utrecht_network_both = ox.graph_from_gdfs(Utrecht_nodes, Utrecht_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Amsterdam_edges.grade.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MULTIPLYING BY LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen:\n",
    "    edges = globals()[f\"{g}_edges\"] \n",
    "\n",
    "    #waterpoints\n",
    "    edges['cost_waterpoints'] = 1\n",
    "    edges['cost_waterpoints'][edges.amenity == 'drinking_water'] = 0\n",
    "\n",
    "    #UHI\n",
    "    edges['cost_UHI'] = edges['UHI_mean'] / 3 #3 comes from the highest UHI_mean value of all the areas\n",
    "\n",
    "    #Greenspace\n",
    "    edges['cost_greenspace'] = 0\n",
    "    edges['cost_greenspace'][(edges['landuse'].isnull()) & (edges['leisure'].isnull()) & (edges['natural'].isnull())] = 1\n",
    "\n",
    "    #slope\n",
    "    edges['cost_grade'] = 0\n",
    "    edges['cost_grade'][(edges['grade'] > 0)] = edges['grade'] / 5.982 #5.982 is the highest grade value of all the areas\n",
    "\n",
    "    #disruptions\n",
    "    edges['cost_disruptions'] = 0\n",
    "    edges['cost_disruptions'][(edges['footway'] == 'crossing') |(edges['cycleway'] == 'crossing') |(edges['barrier_obstnodes_u'] == 'gate')|(edges['barrier_obstnodes_u'] == 'wicket_gate')|(edges['barrier_obstnodes_u'] == 'stile')|(edges['barrier_obstnodes_u'] == 'turnstile')|(edges['barrier_obstnodes_u'] == 'full-height_turnstile')}] = 0.5\n",
    "    edges['cost_disruptions'][(edges['highway_obstnodes_u'] == 'traffic_signals')|(edges['highway_obstnodes_v'] == 'traffic_signals')] = 1\n",
    "    \n",
    "    #surface_unpaved\n",
    "    edges['cost_surface_prefunpaved'] = 0.5\n",
    "    edges['cost_surface_prefunpaved'][edges['surface'].isin(['dirt;grass','grass_paver','mud','dirt','unpaved','wetland','clay','grass','sand','ground','earth','woodchips','reed_bed','soil'])] = 0\n",
    "    edges['cost_surface_prefunpaved'][edges['surface'].isin(['dirt;gravel','crushed_shells','gravel','fine_gravel','shells','schelpen','rubber'])] = 0.25\n",
    "    edges['cost_surface_prefunpaved'][edges['surface'].isin(['compacted','wood','resin','rasin'])] = 0.75\n",
    "    edges['cost_surface_prefunpaved'][edges['surface'].isin(['concrete-slabs','steel','paving_stones;asphalt','concrete:lanes','concrete_slab','concrete_slabs','concrete:deconstructed_czech_hedgehogs','paved','asphalt','metal','marble','concrete:plates','concrete:tiles','concrete','zinc','bricks','brick','tiles','concrete_tiles:30','paving_stones:20','paving_stones:30','metal_grid','asphalt;paving_stones:30x30','stepping_stones','paving_stones','stone','pebblestone','cobblestone','unhewn_cobblestone'])] = 1\n",
    "    \n",
    "    #surface_paved\n",
    "    edges['cost_surface_prefpaved'] = 0.5\n",
    "    edges['cost_surface_prefpaved'][edges['surface'].isin(['dirt;grass','grass_paver','mud','dirt','unpaved','wetland','clay','grass','sand','ground','earth','woodchips','reed_bed','soil'])] = 1\n",
    "    edges['cost_surface_prefpaved'][edges['surface'].isin(['dirt;gravel','crushed_shells','gravel','fine_gravel','shells','schelpen','rubber'])] = 0.75\n",
    "    edges['cost_surface_prefpaved'][edges['surface'].isin(['compacted','wood','resin','rasin'])] = 0.25\n",
    "    edges['cost_surface_prefpaved'][edges['surface'].isin(['concrete-slabs','steel','paving_stones;asphalt','concrete:lanes','concrete_slab','concrete_slabs','concrete:deconstructed_czech_hedgehogs','paved','asphalt','metal','marble','concrete:plates','concrete:tiles','concrete','zinc','bricks','brick','tiles','concrete_tiles:30','paving_stones:20','paving_stones:30','metal_grid','asphalt;paving_stones:30x30','stepping_stones','paving_stones','stone','pebblestone','cobblestone','unhewn_cobblestone'])] = 0\n",
    "    \n",
    "    \n",
    "    globals()[f\"{g}_edges\"] = edges\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SAVE GRAPHS WITH ONLY COST COLUMNS, NO FINAL COST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen:\n",
    "    globals()[f\"{g}_edges\"].to_file(f'data/graphs/retry/with_costs/{g}_edges_with_costs.gpkg', driver = \"GPKG\", layer= 'edges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CALCULATING FINAL COST COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in g_namen:\n",
    "#final cost column\n",
    "    edges['final_cost'] = ((0.3*edges['cost_waterpoints']) + (0.6*edges['cost_UHI']) + (0.8*edges['cost_greenspace']) + (0.2*edges['cost_grade']) + (0.3*edges['cost_disruptions']) + (0.1*edges['cost_surface'])) * edges['length'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get network\n",
    "for g in g_namen:\n",
    "    globals()[f\"{g}_edges\"] = gpd.read_file(f'data/graphs/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.read_file('data/')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "74868beb4ec27fd26d4cf7f91852fff1668a283e4d2efd66b1e888f5eef2fc2a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 ('ox')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
